{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the step by step guide to invest in share market in india?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0].question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with [math]23^{24}[/math] \n",
    "\n",
    "# удалить пустые строки из файлика трейн и перезаписать его\n",
    "\n",
    "# добавить в предобработку -- lemmas\n",
    "# tf-idf\n",
    "# w2vec + n_similarity(ws1, ws2)\n",
    "# vw (vowpal wabbit)\n",
    "# счетчики\n",
    "# биграммы\n",
    "\n",
    "# скрипт должен фититься по трейну и предсказывать на тесте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 'NN'), ('ran', 'VBD'), ('home', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['cat', 'ran', 'home'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tagged_token):\n",
    "    token, tag = tagged_token\n",
    "    return lemmatizer.lemmatize(token, pos=pos_tranform[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ran', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['ran'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'cat', 'home']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lemmatize, nltk.pos_tag(['went', 'cat', 'home'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tranform = {'JJ':'a', 'JJR':'a', 'JJS':'a', 'NN':'n', 'NNS':'n', 'NNP':'n', 'NNPS':'n', 'RB':'r', 'RBR':'r', \n",
    "                'RBS':'r', 'VB':'v', 'VBD':'v', 'VBG':'v', 'VBN':'v', 'VBP':'v', 'VBZ':'v'}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tagged = nltk.pos_tag(words)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(‘dogs’)\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "\n",
    "n for noun files, v for verb files, a for adjective files, r for adverb files.\n",
    "POS tag list:\n",
    "\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "\n",
    "\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "\n",
    "\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tranform = {'JJ':'a', 'JJR':'a', 'JJS':'a', 'NN':'n', 'NNS':'n', 'NNP':'n', 'NNPS':'n', 'RB':'r', 'RBR':'r', \n",
    "                'RBS':'r', 'VB':'v', 'VBD':'v', 'VBG':'v', 'VBN':'v', 'VBP':'v', 'VBZ':'v'}\n",
    "\n",
    "def lemmatize(tagged_token):\n",
    "    token, tag = tagged_token\n",
    "    return lemmatizer.lemmatize(token, pos=pos_tranform.get(tag, 'n'))\n",
    "\n",
    "def preprocessing(text):\n",
    "#     try:\n",
    "    # Dealing with Punctuation and numbers\n",
    "    clean_text = re.sub(\"[^\\w]\", ' ', text)\n",
    "\n",
    "#     lower-case and split into words\n",
    "    clean_text = clean_text.lower().split()\n",
    "    \n",
    "    clean_text = list(map(lemmatize, nltk.pos_tag(clean_text)))\n",
    "#     stopwords\n",
    "    words = [word for word in clean_text if word not in stopwords.words('english')]\n",
    "\n",
    "    return (' '.join(words))\n",
    "#     except:\n",
    "#         print(text)\n",
    "#         return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india?\n",
      "What is the step by step guide to invest in share market in india \n",
      "['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india']\n",
      "['what', 'be', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india']\n",
      "['step', 'step', 'guide', 'invest', 'share', 'market', 'india']\n"
     ]
    }
   ],
   "source": [
    "s = 'What is the step by step guide to invest in share market in india?'\n",
    "print(s)\n",
    "s = re.sub(\"[^\\w]\", ' ', s)\n",
    "print(s)\n",
    "s = clean_text = s.lower().split()\n",
    "print(s)\n",
    "s = list(map(lemmatize, nltk.pos_tag(s)))\n",
    "print(s)\n",
    "s = [word for word in s if word not in stopwords.words('english')]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data = data[~data.question2.isnull()]\n",
    "data = data.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['question1'] = data['question1'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "data['question2'] = data['question2'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>story kohinoor koh noor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder math math divided</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "      <td>fish would survive salt water</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid1  qid2                                          question1  \\\n",
       "id                                                                  \n",
       "0      1     2          step step guide invest share market india   \n",
       "1      3     4                    story kohinoor koh noor diamond   \n",
       "2      5     6       increase speed internet connection using vpn   \n",
       "3      7     8                              mentally lonely solve   \n",
       "4      9    10  one dissolve water quikly sugar salt methane c...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "id                                                                   \n",
       "0                 step step guide invest share market             0  \n",
       "1   would happen indian government stole kohinoor ...             0  \n",
       "2                internet speed increased hacking dns             0  \n",
       "3                    find remainder math math divided             0  \n",
       "4                       fish would survive salt water             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.6308\n",
       "1    0.3692\n",
       "Name: is_duplicate, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.is_duplicate.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(data['is_duplicate'], n_folds=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !mkdir -p f{0..4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37076108733829677\n",
      "0.36774799955476956\n",
      "0.37071161789804347\n",
      "0.3682055975363914\n",
      "0.36858118135945384\n"
     ]
    }
   ],
   "source": [
    "# for fold, (train_ids, test_ids) in enumerate(skf):\n",
    "#     folder = os.path.join('./', 'f{}'.format(fold))\n",
    "#     data.loc[train_ids].to_csv(os.path.join(folder, 'train.csv'))\n",
    "#     data.loc[test_ids].to_csv(os.path.join(folder, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data[['question1', 'question2']], data['is_duplicate'], \n",
    "                                                    test_size=0.3, stratify=data['is_duplicate'],\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155515</th>\n",
       "      <td>best books iit jam mathematics</td>\n",
       "      <td>best books iit jam mathematics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400580</th>\n",
       "      <td>best photo startups</td>\n",
       "      <td>good photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22595</th>\n",
       "      <td>best revenge rejected someone</td>\n",
       "      <td>best revenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15217</th>\n",
       "      <td>story first camera invented</td>\n",
       "      <td>camera invented</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67927</th>\n",
       "      <td>add app vizio smart tv</td>\n",
       "      <td>youtube like apps sharing smart tv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             question1                           question2\n",
       "id                                                                        \n",
       "155515  best books iit jam mathematics      best books iit jam mathematics\n",
       "400580             best photo startups                          good photo\n",
       "22595    best revenge rejected someone                        best revenge\n",
       "15217      story first camera invented                     camera invented\n",
       "67927           add app vizio smart tv  youtube like apps sharing smart tv"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooking Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import paired_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, \n",
    "                             preprocessor = None, stop_words = None, max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concated = pd.concat([x_train['question1'], x_train['question2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_fit = vectorizer.fit(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283001"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine_similarities(model, x_train):\n",
    "    x_train1 = model.transform(x_train['question1']).toarray()\n",
    "    x_train2 = model.transform(x_train['question2']).toarray()\n",
    "    \n",
    "    cosines = []\n",
    "#     cosines!!!!\n",
    "    for i in range(6):\n",
    "        cosines.append(paired_distances(x_train1[50000*i:50000*(i+1)], x_train2[50000*i:50000*(i+1)], metric='cosine'))\n",
    "    \n",
    "    return np.concatenate(cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train1 = vectorizer.transform(x_train['question1']).toarray()\n",
    "x_train2 = vectorizer.transform(x_train['question2']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosines = []\n",
    "for i in range(6):\n",
    "    cosines.append(paired_distances(x_train1[50000*i:50000*(i+1)], x_train2[50000*i:50000*(i+1)], metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine = np.concatenate(cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  0.59175171,  0.29289322, ...,  0.0741799 ,\n",
       "         0.81742581,  0.45566895]),\n",
       " array([ 0.04257289,  0.        ,  1.        , ...,  0.5       ,\n",
       "         1.        ,  0.5       ]),\n",
       " array([ 0.74645372,  0.2       ,  0.26970326, ...,  0.71132487,\n",
       "         0.42264973,  1.        ]),\n",
       " array([ 1.        ,  0.59175171,  1.        , ...,  0.0741799 ,\n",
       "         0.22848325,  0.        ]),\n",
       " array([ 0.18181818,  0.18350342,  0.33333333, ...,  0.1339746 ,\n",
       "         0.25      ,  0.25      ]),\n",
       " array([ 0.42264973,  0.88047714,  0.        , ...,  0.32917961,\n",
       "         0.59175171,  0.42264973])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283001,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283001,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionCV(n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=4, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cosine[:, np.newaxis], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(cosine[:, np.newaxis])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58972109908275994"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70873730927328527"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.658528592318252"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# константный прогноз\n",
    "log_loss(y_train, [y_train.mean()] * len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "# print tfidf_matrix.shape\n",
    "# (4, 11)\n",
    "# Now we have the TF-IDF matrix (tfidf_matrix) for each document (the number of rows of the matrix) with 11 tf-idf terms (the number of columns from the matrix), we can calculate the Cosine Similarity between the first document (“The sky is blue”) with each of the other documents of the set:\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "# array([[ 1.        ,  0.36651513,  0.52305744,  0.13448867]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
